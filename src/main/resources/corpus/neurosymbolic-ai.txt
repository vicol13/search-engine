The amount of data generated by 2020 is estimated to be about 64 zettabytes according to Statista.
By 2025 it is forecasted to be grown to a whopping 180 zettabytes.
Extrapolating these stats, it seems obvious that computing power to generate insights and models out of that data is a catch-22.
One of those ever-growing data streams is unstructured text data. Large Language Models (LLMs) aim to address these problems.
However, there are severe limitations to that approach.
LLM's lack of reproducibility by the scientific community and power requirements makes it dead-end research.
It took 512 V100 GPUs to train MegatronLM that had 45 terabytes of data consuming 27,648 kWh of power which is equal to three years of energy of an average US household.
By 2040, the amount of energy required to train larger models at this rate would require more energy than the entire energy that is powering the earth.
This necessitates research into alternate models with less taxing requirements both on the algorithmic and hardware sides.
MIT-IBM Watson AI Lab is one of the research labs that is trying to solve that problem by proposing a new approach.
This consists of making large BlackBox models more explainable through the symbolic representation of embeddings and the relationship between them.
Let's first assume that ANNs model the neuroanatomy of the human brain in its entirety and machines use perception just as a human child learns cognition by experimenting with its environment.
Famous cognitive psychologist Jean Piaget explains different stages of a child’s cognitive development starting with exploring its environment. While sub-cortical regions of the brain cannot be replicated in machines that are responsible for emotional regulation and memory formation based on biological cells primarily consisting of chemical processes, cortical functions can more or less be emulated.
Earlier works on decoding neural systems are some of the groundbreaking discoveries in the area. Carver Mead’s book ‘Analog VLSI and Neural Systems’ is one good read.
In reinforcement learning terminology these embeddings are called schemas. Once a child learns those schemas, additional information serves as model updates and is called assimilation.
An incongruence with the existing model is accounted for by a process called accommodation which in machine learning terms can be called hyperparameter tuning. These weights can be inhibitory or excitatory in nature.