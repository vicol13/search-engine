Beyond the current news cycle about whether AIs are sentient is a more practical and immediately consequential conversation about AI value alignment: whether and how AIs can be imbued with human values.
Today, this turns on the even more fundamental question of whether the newest generation of language models can or can’t understand concepts — and on what it means to understand.
If, as some researchers contend, language models are mere “babblers” that randomly regurgitate their training data — “garbage in, garbage out” — then real AI value alignment is, at least for now, out of reach.
Seemingly, the best we can do is to carefully curate training inputs to filter out “garbage”, often referred to as “toxic content”, even as we seek to broaden data sources to better represent human diversity.
There are some profound challenges implied here, including governance (who gets to define what is “toxic”?), labor (is it humane to employ people to do “toxic content” filtering?²), and scale (how can we realistically build large models under such constraints?).
This skeptical view also suggests a dubious payoff for the whole language model research program, since the practical value of a mere “babbler” is unclear: what meaningful tasks could a model with no understanding of concepts be entrusted to do? If the answer is none, then why bother with them at all?
On the other hand, if, as I’ll argue here, language models are able to understand concepts, then they’ll have far greater utility — though with this utility, we must also consider a wider landscape of potential harms and risks.
Urgent social and policy questions arise too. When so many of us (myself included) make our living doing information work, what will it mean for the labor market, our economic model, and even our sense of purpose when so many of today’s desk jobs can be automated?
This is no longer a remote, hypothetical prospect, but attention to it has waned as AI denialism has gained traction.
Many AI ethicists have narrowed their focus to the subset of language model problems consistent with the assumption that they understand nothing: their failure to work for digitally underrepresented populations, promulgation of bias, generation of deepfakes, and output of words that might offend.
These are serious issues.
However, today’s AI models are becoming far more generally capable than this narrow focus implies. AI can engineer drugs³ (or poisons⁴), design proteins,⁵ write code,⁶ solve puzzles,⁷ model people’s states of mind,⁸ control robots in human environments,⁹ and plan strategies.¹⁰ These things are hard to dismiss as mere babble; they’ll increasingly involve substantive interactions with people and real outcomes in the world, either for good or for ill.
If AIs are highly capable but malicious, or just clueless about right and wrong, then some of the dangerous outcomes could even resemble those popularized by the very different community of philosophers and researchers who have written, both more sensationally and less groundedly, about AI existential risk.¹¹
It’s becoming increasingly clear that these two disconnected camps in the AI ethics debate are each seeing only part of the picture.
Those who are deeply skeptical about what AI can do haven’t acknowledged either the risk or the potential of the emerging generation of general-purpose AI.
On the other hand, while those in the existential risk camp have been expansive in their articulation of potential harms and benefits, they consider “Artificial General Intelligence” (AGI) to be so distant, mysterious, and inscrutable that it’ll emerge spontaneously in an “intelligence explosion” decades from now;¹² AGI might then proceed, perhaps due to some Douglas Adams-ish programming oversight, to turn the entire universe into paperclips, or worse.¹³
Such doomsday scenarios may have seemed credible in 2014, but they’re far less so now that we’re starting to understand the landscape better. Language modeling has proven to be the key to making the leap from the specialized machine learning applications of the 2010s to the general-purpose AI technology of the 2020s.
he result is hardly an alien entity with inscrutable goals.
Anyone can chat with a language-enabled model, and it can respond in ways so familiar that concern has shifted overnight from worrying about AI’s alienness to worrying about our tendency to anthropomorphize it.
It’s all too human-like!
Although anthropomorphism does pose its own risks,¹⁴ this familiarity is good news, in that it may make human value alignment far more straightforward than the existential risk community has imagined.
This is because, although our biology endows us with certain pre-linguistic moral sentiments (such as care for offspring and in-group altruism, both of which we share with many other species), language generalizes these sentiments into ethical values, whether widely held or aspirational.
Hence oral and written language have mediated the fields of ethics, moral philosophy, law, and religion for thousands of years.
For an AI model to behave according to a given set of ethical values, it has to be able to understand what those values are just as we would — via language.
By sharing language with AIs, we can share norms and values with them too.
We have early evidence that this approach works, and as language-enabled models improve generally, so too will their ability to behave according to ethical principles.
This is the main point I hope to convey in this essay.
In itself, the ability to endow an AI with values isn’t a panacea.
It doesn’t guarantee perfect judgment — an unrealistic goal for either human or machine.
Nor does it address governance questions: who gets to define an AI’s values, and how much scope will these have for personal or cultural variation? Are some values better than others? How should AIs, their creators, and their users be held morally accountable? Neither does it tackle the economic problem articulated by John Maynard Keynes in 1930 — how to equitably distribute the collective gains of increasing automation,¹⁵ soon to include much intellectual labor.
What it does offer is a clear route to imbuing an AI with values that are transparent, legible, and controllable by ordinary people.
It also suggests mechanisms for addressing the narrower issues of bias and underrepresentation within the same framework.
My view is that AI values needn’t be — and shouldn’t be — dictated by engineers, ethicists, lawyers, or any other narrow constituency.
Neither should they remain bulleted lists of desiderata posted on the web pages of standards bodies, governments, or corporations, with no direct connection to running code.
They should, instead, become the legible and auditable “operating handbooks” of tomorrow’s AIs.